# Covariate development

## Foundational DEM
There are three potential sources to use for the foundational DEM, all based on publically available (LiDAR for Minnesota)[https://www.mngeo.state.mn.us/chouse/elevation/lidar.html]:

- [2008-2012 Statewide Lidar collection](https://www.mngeo.state.mn.us/chouse/elevation/lidar_2008-2012.html) - this is the 1m and 3m data available through MnTOPO but cannot be massively acquired from MnTOPO (only for smaller regions). This is most easily acquired from the [ftp site](https://resources.gisdata.mn.gov/pub/data/elevation/lidar/). Here you can download .gdb files that contain the 1m and 3m data (by county or by 250k quads). The problem with all of these is that they are distributed as .gdbs so need to be freed first in ArcPro as geotiffs. This is what Kellen has been working on - we are putting these geotiffs direct onto Tier2. Advantage? This is complete across the state.

- 2021-2025 Statewide Lidar collection. This is what is provided as a GEE resource through USGS 3DEP. Advantages: this can easily be cut up, batched, and tiled any way we want in a much more automated way. 1m data available through 3DEP. Disadvantages: this is NOT COMPLETE across the state. Here is the [current status](https://experience.arcgis.com/experience/dab2506c98f24eb1a8c8f69061423516/); NOTE: even when the data is reviewed and complete there will remain some redacted areas in this version so will not be statewide without gaps. I have GEE scripts to do this on a HUC8 basis.

-3DEP 10m. Available through GEE. This is complete but there are lots of artifacts in the straight 10m version. Better to use the 1m and aggregate in GEE to 10m. Much better product that way.

## Attempts to construct tiled 10m DEM.

Trying to pull10m data (which is complete across the state) from 3dep using GEE. Tried the 10m product but there are a lot of artifacts. So now going back to the 1m product through GEE (Which is the updated LiDAR product and not the original product)- it appears that using the 1m product and aggregating it will result in a better smoother product without artifcats rather than using the straight 3dep10m product.

First, trying to go through GEE. In order to process hydrological variables, trying to get DEMs on HUC4 basis. So need to download all of the 1m tiles from GEE hor a given HUC 4. Turns out there are too many tiles for a HUC-4 and GEE can't process on the fly. SO - instead I am going to use Colby's approach and use HUC-8 watersheds.

### 2008-2012

I downloaded all of the county level gdbs in an automated way here:
# Loop through counties and download the elevation data file
for county in "${COUNTIES[@]}"; do
    ((current_count++))
    
    echo "[$current_count/$total_counties] Downloading: $county..."
    
    wget --progress=bar:force -nc "${BASE_URL}/${county}/elevation_data.gdb.zip" -P mn_lidar_data/ 2>&1 | \
    tee lidar_data/${county}_download.log
    
    echo "[$current_count/$total_counties] Finished: $county"
done

echo "All downloads completed."




NEED TO REDOWNLOAD CROW WING

#### NEW COMPRRECTED SCRIPT:
#!/bin/bash

# Correct Base URL for Minnesota LiDAR data
BASE_URL="https://resources.gisdata.mn.gov/pub/data/elevation/lidar/county"

# List of counties
COUNTIES=("aitkin" "anoka" "becker" "beltrami" "benton" "bigstone" "blueearth" "brown" "carlton" "carver" "cass" "chippewa" "chisago" "clay" "clearwater" "cook" "cottonwood" "crowwing" "dakota" "dodge" "douglas" "faribault" "fillmore" "freeborn" "goodhue" "grant" "hennepin" "houston" "hubbard" "isanti" "itasca" "jackson" "kanabec" "kandiyohi" "kittson" "koochiching" "lacquiparle" "lake" "lakeofthewoods" "lesueur" "lincoln" "lyon" "mahnomen" "marshall" "martin" "mcleod" "meeker" "millelacs" "morrison" "mower" "murray" "nicollet" "nobles" "norman" "olmsted" "ottertail" "pennington" "pine" "pipestone" "polk" "pope" "ramsey" "redlake" "redwood" "renville" "rice" "rock" "roseau" "scott" "sherburne" "sibley" "stearns" "steele" "stevens" "stlouis" "swift" "todd" "traverse" "wabasha" "wadena" "waseca" "washington" "watonwan" "wilkin" "winona" "wright" "yellowmedicine")

# Create directories for downloads and logs
mkdir -p lidar_data logs

# Track progress
total_count=${#COUNTIES[@]}
current_count=0

# Loop through counties
for county in "${COUNTIES[@]}"; do
    ((current_count++))
    
    # Define the output file path
    output_file="mn_lidar_data/elevation_data_${county}.gdb.zip"

    # Check if the file already exists
    if [ -f "$output_file" ]; then
        echo "[$current_count/$total_count] ⏭️ Skipping $county (Already downloaded: $output_file)"
        continue
    fi

    echo "[$current_count/$total_count] Downloading: $county..."

    # Attempt to download file and rename it
    wget --progress=bar:force -O "$output_file" "${BASE_URL}/${county}/elevation_data.gdb.zip" 2>&1 | tee logs/${county}_download.log

    # Check if download succeeded
    if [ $? -eq 0 ]; then
        echo "[$current_count/$total_count] ✅ Successfully downloaded: $county as elevation_data_${county}.gdb.zip"
    else
        echo "[$current_count/$total_count] ❌ Failed to download: $county (Check logs/${county}_download.log)"
    fi
done

echo "All downloads completed."



#### CHECK THAT FILE SIZES MATCH AFTER FULL DOWNLOAD
#!/bin/bash

# Base URL for Minnesota LiDAR data
BASE_URL="https://resources.gisdata.mn.gov/pub/data/elevation/lidar/county"

# List of counties
COUNTIES=("aitkin" "anoka" "becker" "beltrami" "benton" "bigstone" "blueearth" "brown" "carlton" "carver" "cass" "chippewa" "chisago" "clay" "clearwater" "cook" "cottonwood" "crowwing" "dakota" "dodge" "douglas" "faribault" "fillmore" "freeborn" "goodhue" "grant" "hennepin" "houston" "hubbard" "isanti" "itasca" "jackson" "kanabec" "kandiyohi" "kittson" "koochiching" "lacquiparle" "lake" "lakeofthewoods" "lesueur" "lincoln" "lyon" "mahnomen" "marshall" "martin" "mcleod" "meeker" "millelacs" "morrison" "mower" "murray" "nicollet" "nobles" "norman" "olmsted" "ottertail" "pennington" "pine" "pipestone" "polk" "pope" "ramsey" "redlake" "redwood" "renville" "rice" "rock" "roseau" "scott" "sherburne" "sibley" "stearns" "steele" "stevens" "stlouis" "swift" "todd" "traverse" "wabasha" "wadena" "waseca" "washington" "watonwan" "wilkin" "winona" "wright" "yellowmedicine")

# Create a directory for logs
mkdir -p logs

# Track progress
total_count=${#COUNTIES[@]}
current_count=0

# Loop through counties and check file sizes
for county in "${COUNTIES[@]}"; do
    ((current_count++))
    
    # Define local file path
    local_file="./mn_lidar_data/elevation_data_${county}.gdb.zip"

    # Check if file exists locally
    if [ ! -f "$local_file" ]; then
        echo "[$current_count/$total_count] ⚠️ Missing file: $county (lidar_data/elevation_data_${county}.gdb.zip)"
        continue
    fi

    # Get the local file size in bytes
    local_size=$(stat -c%s "$local_file")

    # Get the expected file size from the HTTP site
    remote_size=$(curl -sI "${BASE_URL}/${county}/elevation_data.gdb.zip" | grep -i "Content-Length" | awk '{print $2}' | tr -d '\r')

    # Check if the remote size is available
    if [[ -z "$remote_size" ]]; then
        echo "[$current_count/$total_count] ❓ Could not retrieve remote file size for $county"
        continue
    fi

    # Compare local and remote file sizes
    if [[ "$local_size" -eq "$remote_size" ]]; then
        echo "[$current_count/$total_count] ✅ File size matches for $county"
    else
        echo "[$current_count/$total_count] ❌ Size mismatch for $county: (Local: $local_size bytes, Remote: $remote_size bytes)"
    fi
done

echo "File size verification completed."







###CORRECTED BELOW:

# Correct Base URL for Minnesota LiDAR data
BASE_URL="https://resources.gisdata.mn.gov/pub/data/elevation/lidar/county"

# List of counties
COUNTIES=("aitkin" "anoka" "becker" "beltrami" "benton" "bigstone" "blueearth" "brown" "carlton" "carver" "cass" "chippewa" "chisago" "clay" "clearwater" "cook" "cottonwood" "crowwing" "dakota" "dodge" "douglas" "faribault" "fillmore" "freeborn" "goodhue" "grant" "hennepin" "houston" "hubbard" "isanti" "itasca" "jackson" "kanabec" "kandiyohi" "kittson" "koochiching" "lacquiparle" "lake" "lakeofthewoods" "lesueur" "lincoln" "lyon" "mahnomen" "marshall" "martin" "mcleod" "meeker" "millelacs" "morrison" "mower" "murray" "nicollet" "nobles" "norman" "olmsted" "ottertail" "pennington" "pine" "pipestone" "polk" "pope" "ramsey" "redlake" "redwood" "renville" "rice" "rock" "roseau" "scott" "sherburne" "sibley" "stearns" "steele" "stevens" "stlouis" "swift" "todd" "traverse" "wabasha" "wadena" "waseca" "washington" "watonwan" "wilkin" "winona" "wright" "yellowmedicine")

# Create directories for downloads and logs
mkdir -p lidar_data logs

# Track progress
total_count=${#COUNTIES[@]}
current_count=0

# Loop through counties
for county in "${COUNTIES[@]}"; do
    ((current_count++))
    
    echo "[$current_count/$total_count] Downloading: $county..."

    # Define the output file path
    output_file="mn_lidar_data/elevation_data_${county}.gdb.zip"

    # Attempt to download file and rename it
    wget --progress=bar:force -nc -O "$output_file" "${BASE_URL}/${county}/elevation_data.gdb.zip" 2>&1 | tee logs/${county}_download.log

    # Check if download succeeded
    if [ $? -eq 0 ]; then
        echo "[$current_count/$total_count] ✅ Successfully downloaded: $county as elevation_data_${county}.gdb.zip"
    else
        echo "[$current_count/$total_count] ❌ Failed to download: $county (Check logs/${county}_download.log)"
    fi
done

echo "All downloads completed."


REMOVE CROW WING FOR REDOWNLOAD
rm ./mn_lidar_data/elevation_data_crowwing.gdb.zip

Then run script again

**The problem is that these are all in .gdb and even with GDAL you cannot free them in Python. They can ONLY be freed manually in ArcPro**!!!!

So, Kellen is working in parallel and freeing all of the 2008-2012 data which we will eventually upload and work with on MSI.

### DEM derivatives

## Imagery

- Used GEE to grab a sentinel2 median summer composite for 50km tiles across the state (2019-2024) cloud free. More work to do on this but at least I have it. It is all currently sitting on Google Cloud storage and needs to be pulled down to MSI Tier 2.
